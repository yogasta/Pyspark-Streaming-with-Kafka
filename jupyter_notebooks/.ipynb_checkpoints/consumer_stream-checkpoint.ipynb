{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f327097b-87cf-41a0-a082-d86d18bc233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, sum, to_timestamp, window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from datetime import datetime\n",
    "import psycopg2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1afec34c-d58b-4d13-b678-473f5e2c9515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.kafka#kafka-clients added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ef693dfe-945e-4c3c-9dbf-d3fd95dce410;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.9-1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      ":: resolution report :: resolve 162ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.9-1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.apache.kafka#kafka-clients;2.6.0 by [org.apache.kafka#kafka-clients;2.8.0] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   10  |   0   |   0   |   1   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ef693dfe-945e-4c3c-9dbf-d3fd95dce410\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/4ms)\n",
      "24/09/08 12:02:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PurchaseEventStreaming\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2,org.apache.kafka:kafka-clients:2.8.0\") \\\n",
    "    .config(\"spark.streaming.kafka.consumer.cache.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.streaming.schemaInference\", \"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbcc5d5c-6b8b-4e00-b414-6675d0695236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for purchase events\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", StringType()),\n",
    "    StructField(\"product_id\", IntegerType()),\n",
    "    StructField(\"quantity\", IntegerType()),\n",
    "    StructField(\"price\", DoubleType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1000e565-aafb-40fe-b371-c196747fc82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_broker = os.environ.get('KAFKA_BROKER')\n",
    "kafka_topic = \"purchase_event\"\n",
    "\n",
    "# PostgreSQL connection details\n",
    "pg_host = os.environ.get('POSTGRES_HOST')\n",
    "pg_db = os.environ.get('POSTGRES_DB')\n",
    "pg_user = os.environ.get('POSTGRES_USER')\n",
    "pg_password = os.environ.get('POSTGRES_PASSWORD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09f90563-721e-444a-a905-0454640d971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_postgres(df, epoch_id):\n",
    "    # Create a connection to PostgreSQL\n",
    "    conn = psycopg2.connect(host=pg_host, database=pg_db, user=pg_user, password=pg_password)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Create table if not exists\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS running_total (\n",
    "        timestamp TIMESTAMP PRIMARY KEY,\n",
    "        running_total DOUBLE PRECISION\n",
    "    )\n",
    "    \"\"\")\n",
    "\n",
    "    # Insert or update data\n",
    "    print(f\"\\n--- Running Total Updated at {datetime.now()} ---\")\n",
    "    print(\"Timestamp | Running Total\")\n",
    "    print(\"-----------+---------------\")\n",
    "    for row in df.collect():\n",
    "        window_end = row.window_end\n",
    "        running_total = row.running_total\n",
    "        \n",
    "        if window_end is not None and running_total is not None:\n",
    "            cur.execute(\"\"\"\n",
    "            INSERT INTO running_total (timestamp, running_total)\n",
    "            VALUES (%s, %s)\n",
    "            ON CONFLICT (timestamp) DO UPDATE\n",
    "            SET running_total = EXCLUDED.running_total\n",
    "            \"\"\", (window_end, running_total))\n",
    "            \n",
    "            print(f\"{window_end} | {running_total:.2f}\")\n",
    "        else:\n",
    "            print(f\"Skipping row due to None values: window_end={window_end}, running_total={running_total}\")\n",
    "\n",
    "    # Commit and close\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdb3b2c1-9a22-45a5-a38b-b48ab03c9402",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kafka_broker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Read streaming data from Kafka\u001b[39;00m\n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mreadStream \\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka.bootstrap.servers\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mkafka_broker\u001b[49m) \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubscribe\u001b[39m\u001b[38;5;124m\"\u001b[39m, kafka_topic) \\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstartingOffsets\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mearliest\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka.security.protocol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPLAINTEXT\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Parse JSON from value and process\u001b[39;00m\n\u001b[1;32m     11\u001b[0m parsed_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselect(\n\u001b[1;32m     12\u001b[0m     from_json(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m), schema)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m )\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.*\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kafka_broker' is not defined"
     ]
    }
   ],
   "source": [
    "# Read streaming data from Kafka\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"kafka.security.protocol\", \"PLAINTEXT\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse JSON from value and process\n",
    "parsed_df = df.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")\n",
    ").select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d28476e3-b3ca-427a-b4ff-b751655a1509",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parsed_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Convert timestamp to proper format and calculate running total\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m sales_df \u001b[38;5;241m=\u001b[39m \u001b[43mparsed_df\u001b[49m \\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m, to_timestamp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msales\u001b[39m\u001b[38;5;124m\"\u001b[39m, col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantity\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m*\u001b[39m col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mgroupBy(window(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1 day\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39magg(\u001b[38;5;28msum\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msales\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrunning_total\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \\\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39mselect(\n\u001b[1;32m      8\u001b[0m         col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow.end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow_end\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      9\u001b[0m         col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrunning_total\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parsed_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert timestamp to proper format and calculate running total\n",
    "sales_df = parsed_df \\\n",
    "    .withColumn(\"timestamp\", to_timestamp(\"timestamp\")) \\\n",
    "    .withColumn(\"sales\", col(\"quantity\") * col(\"price\")) \\\n",
    "    .groupBy(window(\"timestamp\", \"1 day\")) \\\n",
    "    .agg(sum(\"sales\").alias(\"running_total\")) \\\n",
    "    .select(\n",
    "        col(\"window.end\").alias(\"window_end\"),\n",
    "        col(\"running_total\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cee0ee4d-064a-45d4-a5d0-b3fe2ed92ce5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sales_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Start the streaming query\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[43msales_df\u001b[49m\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mforeachBatch(save_to_postgres) \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mtrigger(processingTime\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10 seconds\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m      8\u001b[0m query\u001b[38;5;241m.\u001b[39mawaitTermination()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sales_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Start the streaming query\n",
    "query = sales_df.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .foreachBatch(save_to_postgres) \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b45512-d03a-4a10-9362-88d770f2ea8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
