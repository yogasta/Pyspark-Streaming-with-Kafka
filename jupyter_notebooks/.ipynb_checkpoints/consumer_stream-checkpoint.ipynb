{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f327097b-87cf-41a0-a082-d86d18bc233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, sum, to_timestamp, window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from datetime import datetime\n",
    "import psycopg2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1afec34c-d58b-4d13-b678-473f5e2c9515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.kafka#kafka-clients added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-64ecf6b3-c469-4ae9-a78d-de134af9cd81;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.9-1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.1.2/spark-sql-kafka-0-10_2.12-3.1.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2!spark-sql-kafka-0-10_2.12.jar (668ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.8.0/kafka-clients-2.8.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;2.8.0!kafka-clients.jar (1123ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.1.2/spark-token-provider-kafka-0-10_2.12-3.1.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2!spark-token-provider-kafka-0-10_2.12.jar (389ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.6.2/commons-pool2-2.6.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.6.2!commons-pool2.jar (392ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (389ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/luben/zstd-jni/1.4.9-1/zstd-jni-1.4.9-1.jar ...\n",
      "\t[SUCCESSFUL ] com.github.luben#zstd-jni;1.4.9-1!zstd-jni.jar (8002ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.7.1/lz4-java-1.7.1.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.7.1!lz4-java.jar (422ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.8.1/snappy-java-1.1.8.1.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.8.1!snappy-java.jar(bundle) (496ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.30!slf4j-api.jar (391ms)\n",
      ":: resolution report :: resolve 13015ms :: artifacts dl 12277ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.9-1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.apache.kafka#kafka-clients;2.6.0 by [org.apache.kafka#kafka-clients;2.8.0] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   10  |   9   |   9   |   1   ||   9   |   9   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-64ecf6b3-c469-4ae9-a78d-de134af9cd81\n",
      "\tconfs: [default]\n",
      "\t9 artifacts copied, 0 already retrieved (13968kB/10ms)\n",
      "24/09/08 12:54:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PurchaseEventStreaming\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2,org.apache.kafka:kafka-clients:2.8.0\") \\\n",
    "    .config(\"spark.streaming.kafka.consumer.cache.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.streaming.schemaInference\", \"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbcc5d5c-6b8b-4e00-b414-6675d0695236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for purchase events\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", StringType()),\n",
    "    StructField(\"product_id\", IntegerType()),\n",
    "    StructField(\"quantity\", IntegerType()),\n",
    "    StructField(\"price\", DoubleType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1000e565-aafb-40fe-b371-c196747fc82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_broker = os.environ.get('KAFKA_BROKER')\n",
    "kafka_topic = \"purchase_event\"\n",
    "\n",
    "# PostgreSQL connection details\n",
    "pg_host = os.environ.get('POSTGRES_HOST')\n",
    "pg_db = os.environ.get('POSTGRES_DB')\n",
    "pg_user = os.environ.get('POSTGRES_USER')\n",
    "pg_password = os.environ.get('POSTGRES_PASSWORD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09f90563-721e-444a-a905-0454640d971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_postgres(df, epoch_id):\n",
    "    # Create a connection to PostgreSQL\n",
    "    conn = psycopg2.connect(host=pg_host, database=pg_db, user=pg_user, password=pg_password)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Create table if not exists\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS running_total (\n",
    "        timestamp TIMESTAMP PRIMARY KEY,\n",
    "        running_total DOUBLE PRECISION\n",
    "    )\n",
    "    \"\"\")\n",
    "\n",
    "    # Insert or update data\n",
    "    print(f\"\\n--- Running Total Updated at {datetime.now()} ---\")\n",
    "    print(\"Timestamp | Running Total\")\n",
    "    print(\"-----------+---------------\")\n",
    "    for row in df.collect():\n",
    "        window_end = row.window_end\n",
    "        running_total = row.running_total\n",
    "        \n",
    "        if window_end is not None and running_total is not None:\n",
    "            cur.execute(\"\"\"\n",
    "            INSERT INTO running_total (timestamp, running_total)\n",
    "            VALUES (%s, %s)\n",
    "            ON CONFLICT (timestamp) DO UPDATE\n",
    "            SET running_total = EXCLUDED.running_total\n",
    "            \"\"\", (window_end, running_total))\n",
    "            \n",
    "            print(f\"{window_end} | {running_total:.2f}\")\n",
    "        else:\n",
    "            print(f\"Skipping row due to None values: window_end={window_end}, running_total={running_total}\")\n",
    "\n",
    "    # Commit and close\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdb3b2c1-9a22-45a5-a38b-b48ab03c9402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read streaming data from Kafka\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"kafka.security.protocol\", \"PLAINTEXT\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse JSON from value and process\n",
    "parsed_df = df.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")\n",
    ").select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d28476e3-b3ca-427a-b4ff-b751655a1509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to proper format and calculate running total for 1 day (you can change window by the preferred time you want)\n",
    "sales_df = parsed_df \\\n",
    "    .withColumn(\"timestamp\", to_timestamp(\"timestamp\")) \\\n",
    "    .withColumn(\"sales\", col(\"quantity\") * col(\"price\")) \\\n",
    "    .groupBy(window(\"timestamp\", \"1 day\")) \\\n",
    "    .agg(sum(\"sales\").alias(\"running_total\")) \\\n",
    "    .select(\n",
    "        col(\"window.end\").alias(\"window_end\"),\n",
    "        col(\"running_total\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee0ee4d-064a-45d4-a5d0-b3fe2ed92ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/08 12:54:21 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-23602c73-8676-4564-91d0-fd999a751137. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/09/08 12:54:21 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/09/08 12:54:21 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "24/09/08 12:54:21 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "24/09/08 12:54:21 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "24/09/08 12:54:21 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "24/09/08 12:54:21 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Total Updated at 2024-09-08 12:54:22.866591 ---\n",
      "Timestamp | Running Total\n",
      "-----------+---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-09 00:00:00 | 26460.79\n",
      "\n",
      "\n",
      "\n",
      "--- Running Total Updated at 2024-09-08 12:54:30.112054 ---\n",
      "Timestamp | Running Total\n",
      "-----------+---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-09 00:00:00 | 32681.36\n",
      "\n",
      "\n",
      "\n",
      "--- Running Total Updated at 2024-09-08 12:54:40.086570 ---\n",
      "Timestamp | Running Total\n",
      "-----------+---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-09 00:00:00 | 33030.12\n",
      "\n",
      "\n",
      "\n",
      "--- Running Total Updated at 2024-09-08 12:54:50.085703 ---\n",
      "Timestamp | Running Total\n",
      "-----------+---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-09 00:00:00 | 34682.54\n",
      "\n",
      "\n",
      "\n",
      "--- Running Total Updated at 2024-09-08 12:55:00.083290 ---\n",
      "Timestamp | Running Total\n",
      "-----------+---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-09 00:00:00 | 37394.41\n",
      "\n",
      "\n",
      "\n",
      "--- Running Total Updated at 2024-09-08 12:55:10.092295 ---\n",
      "Timestamp | Running Total\n",
      "-----------+---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-09 00:00:00 | 39462.16\n",
      "\n",
      "\n",
      "\n",
      "--- Running Total Updated at 2024-09-08 12:55:20.088150 ---\n",
      "Timestamp | Running Total\n",
      "-----------+---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-09 00:00:00 | 40452.38\n",
      "\n",
      "\n",
      "\n",
      "--- Running Total Updated at 2024-09-08 12:55:30.069380 ---\n",
      "Timestamp | Running Total\n",
      "-----------+---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-09 00:00:00 | 46331.97\n",
      "\n",
      "\n",
      "\n",
      "--- Running Total Updated at 2024-09-08 12:55:40.075821 ---\n",
      "Timestamp | Running Total\n",
      "-----------+---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-09 00:00:00 | 49176.69\n",
      "\n",
      "\n",
      "\n",
      "--- Running Total Updated at 2024-09-08 12:55:50.073820 ---\n",
      "Timestamp | Running Total\n",
      "-----------+---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-09 00:00:00 | 51398.69\n",
      "\n",
      "\n",
      "\n",
      "--- Running Total Updated at 2024-09-08 12:56:00.065750 ---\n",
      "Timestamp | Running Total\n",
      "-----------+---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-09 00:00:00 | 60196.93\n",
      "\n",
      "\n",
      "\n",
      "--- Running Total Updated at 2024-09-08 12:56:10.080801 ---\n",
      "Timestamp | Running Total\n",
      "-----------+---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-09 00:00:00 | 66645.25\n",
      "\n",
      "\n",
      "\n",
      "--- Running Total Updated at 2024-09-08 12:56:20.063057 ---\n",
      "Timestamp | Running Total\n",
      "-----------+---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-09 00:00:00 | 68033.95\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start the streaming query\n",
    "query = sales_df.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .foreachBatch(save_to_postgres) \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b45512-d03a-4a10-9362-88d770f2ea8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
